{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device parallelism for SPMD in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CudaDevice(id=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = jnp.array([2,3,4], device=jax.devices()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CudaDevice(id=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When creating jax arrays from scratch, you also need to create it's sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleDeviceSharding(device=CudaDevice(id=0), memory_kind=device)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────┐\n",
       "│ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> │\n",
       "└───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌───────┐\n",
       "│ GPU \u001b[1;36m0\u001b[0m │\n",
       "└───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────┐\n",
       "│ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> │\n",
       "└───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌───────┐\n",
       "│ GPU \u001b[1;36m1\u001b[0m │\n",
       "└───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an array with a non-trivial sharding, you can define a jax.sharding specification for the array and pass this to jax.device_put()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedSharding(mesh=Mesh('x': 3), spec=PartitionSpec('x',), memory_kind=device)\n"
     ]
    }
   ],
   "source": [
    "mesh = jax.make_mesh((3,), ('x',))\n",
    "sharding = jax.sharding.NamedSharding(mesh, PartitionSpec('x',))\n",
    "print(sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = jnp.arange(24.0).reshape(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_sharded = jax.device_put(arr, sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, arr_sharded's data is split across 3 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedSharding(mesh=Mesh('x': 3), spec=PartitionSpec('x',), memory_kind=device)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_sharded.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────────────────────────────────────────────────────────────┐\n",
       "│                             GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                             │\n",
       "│                                                               │\n",
       "├───────────────────────────────────────────────────────────────┤\n",
       "│                             GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                             │\n",
       "│                                                               │\n",
       "├───────────────────────────────────────────────────────────────┤\n",
       "│                             GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                             │\n",
       "│                                                               │\n",
       "└───────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌───────────────────────────────────────────────────────────────┐\n",
       "│                             GPU \u001b[1;36m0\u001b[0m                             │\n",
       "│                                                               │\n",
       "├───────────────────────────────────────────────────────────────┤\n",
       "│                             GPU \u001b[1;36m1\u001b[0m                             │\n",
       "│                                                               │\n",
       "├───────────────────────────────────────────────────────────────┤\n",
       "│                             GPU \u001b[1;36m2\u001b[0m                             │\n",
       "│                                                               │\n",
       "└───────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(arr_sharded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automatic parallelism via JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have partitioned your data, the easiest way to do parallel computation is to simply pass the data to jax.jit compiled function. In jax, you only need to specify how you want your input and output to be partitioned, and the compiler will figure out how to 1. Partition everything inside, and 2. Compile inter-device communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "    return 2 * jnp.sin(x) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = f(arr_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────────────────────────────────────────────────────────────┐\n",
       "│                             GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                             │\n",
       "│                                                               │\n",
       "├───────────────────────────────────────────────────────────────┤\n",
       "│                             GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                             │\n",
       "│                                                               │\n",
       "├───────────────────────────────────────────────────────────────┤\n",
       "│                             GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                             │\n",
       "│                                                               │\n",
       "└───────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌───────────────────────────────────────────────────────────────┐\n",
       "│                             GPU \u001b[1;36m0\u001b[0m                             │\n",
       "│                                                               │\n",
       "├───────────────────────────────────────────────────────────────┤\n",
       "│                             GPU \u001b[1;36m1\u001b[0m                             │\n",
       "│                                                               │\n",
       "├───────────────────────────────────────────────────────────────┤\n",
       "│                             GPU \u001b[1;36m2\u001b[0m                             │\n",
       "│                                                               │\n",
       "└───────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F is an element-wise function. Each accelerator operates on the shard it holds and the output is sharded the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_sharded.sharding == result.sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_sharded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "    return x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = f(arr_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌─────────┐\n",
       "│GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>│\n",
       "└─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌─────────┐\n",
       "│GPU \u001b[1;36m0\u001b[0m,\u001b[1;36m1\u001b[0m,\u001b[1;36m2\u001b[0m│\n",
       "└─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is replicated since each GPU must communicate to get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 28.,  92., 156.], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌───────┬───────┬───────┐\n",
       "│ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> │ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> │ GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> │\n",
       "└───────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌───────┬───────┬───────┐\n",
       "│ GPU \u001b[1;36m0\u001b[0m │ GPU \u001b[1;36m1\u001b[0m │ GPU \u001b[1;36m2\u001b[0m │\n",
       "└───────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "    return x.sum(axis=1)\n",
    "result = f(arr_sharded)\n",
    "jax.debug.visualize_array_sharding(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, each result is held individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semi-automated sharding with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want some control over output shard, you can define it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "    out = x.sum(axis=0)\n",
    "    mesh = jax.make_mesh((3,), ('x',))\n",
    "    sharding = jax.sharding.NamedSharding(mesh, PartitionSpec('x'))\n",
    "    return jax.lax.with_sharding_constraint(out, sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = f(arr_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌─────────┐\n",
       "│GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>│\n",
       "└─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌─────────┐\n",
       "│GPU \u001b[1;36m0\u001b[0m,\u001b[1;36m1\u001b[0m,\u001b[1;36m2\u001b[0m│\n",
       "└─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual Partitioning with shard_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jax.experimental.shard_map.shard_map() works by mapping a function across a particular mesh of devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.shard_map import shard_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = jax.make_mesh((3,), ('x',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for a given shard\n",
    "@jax.jit\n",
    "def f_elementwise(x):\n",
    "  jax.debug.print(str(x.shape))\n",
    "  return 2 * jnp.sin(x) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_elementwise_sharded = shard_map(\n",
    "    f_elementwise, \n",
    "    mesh=mesh,\n",
    "    in_specs=PartitionSpec('x'),\n",
    "    out_specs = PartitionSpec('x')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = jnp.arange(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "(8,)\n",
      "(8,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([ 1.        ,  2.682942  ,  2.818595  ,  1.28224   , -0.513605  ,\n",
       "       -0.9178486 ,  0.44116902,  2.3139732 ,  2.9787164 ,  1.824237  ,\n",
       "       -0.08804214, -0.99998045, -0.07314587,  1.8403342 ,  2.9812148 ,\n",
       "        2.3005757 ,  0.42419332, -0.92279494, -0.50197446,  1.2997544 ,\n",
       "        2.8258905 ,  2.6733112 ,  0.98229736, -0.69244087], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_elementwise_sharded(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it can be observed, the function only sees \"its\" shard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore, aggregation operations like sum operate independently. If you want it across devices, you need to use something like jax.lax.psum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  sum_in_shard = x.sum()\n",
    "  return jax.lax.psum(sum_in_shard, 'x')\n",
    "\n",
    "result = shard_map(f, mesh=mesh, in_specs=PartitionSpec('x'), out_specs=PartitionSpec())(arr_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(276., dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedSharding(mesh=Mesh('x': 3), spec=PartitionSpec(), memory_kind=device)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using jax.jit, if you shard the leading axis of both x and weights in the same way, then the matrix multiplication will automatically happen in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can use jax.lax.with_sharding_constraint() in the function to automatically distribute unsharded inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
