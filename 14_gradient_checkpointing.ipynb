{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the jax.checkpoint() decorator (aliased as jax.remat()) with jax.grad() to control which intermediates are saved on the forward pass versus the recomputed intermediates on the backward pass, trading off memory and FLOPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f32[5,4] from the argument W1\n",
      "f32[6,5] from the argument W2\n",
      "f32[7,6] from the argument W3\n",
      "f32[4] from the argument x\n",
      "f32[5] output of sin from /tmp/ipykernel_3842389/194814268.py:6 (g)\n",
      "f32[5] output of cos from /tmp/ipykernel_3842389/194814268.py:6 (g)\n",
      "f32[6] output of sin from /tmp/ipykernel_3842389/194814268.py:6 (g)\n",
      "f32[6] output of cos from /tmp/ipykernel_3842389/194814268.py:6 (g)\n",
      "f32[7] output of cos from /tmp/ipykernel_3842389/194814268.py:6 (g)\n"
     ]
    }
   ],
   "source": [
    "import jax.ad_checkpoint\n",
    "\n",
    "\n",
    "def g(W, x):\n",
    "    y = jnp.dot(W, x)\n",
    "    return jnp.sin(y)\n",
    "\n",
    "def f(W1, W2, W3, x):\n",
    "    x = g(W1, x)\n",
    "    x = g(W2, x)\n",
    "    x = g(W3, x)\n",
    "    return x\n",
    "\n",
    "W1 = jnp.ones((5, 4))\n",
    "W2 = jnp.ones((6, 5))\n",
    "W3 = jnp.ones((7, 6))\n",
    "x = jnp.ones(4)\n",
    "\n",
    "jax.ad_checkpoint.print_saved_residuals(f, W1, W2, W3, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f32[5,4] from the argument W1\n",
      "f32[6,5] from the argument W2\n",
      "f32[7,6] from the argument W3\n",
      "f32[4] from the argument x\n",
      "f32[5] output of sin from /tmp/ipykernel_3842389/2411689849.py:6 (g)\n",
      "f32[6] output of sin from /tmp/ipykernel_3842389/2411689849.py:6 (g)\n"
     ]
    }
   ],
   "source": [
    "import jax.ad_checkpoint\n",
    "\n",
    "\n",
    "def g(W, x):\n",
    "    y = jnp.dot(W, x)\n",
    "    return jnp.sin(y)\n",
    "\n",
    "def f(W1, W2, W3, x):\n",
    "    x = jax.checkpoint(g)(W1, x)\n",
    "    x = jax.checkpoint(g)(W2, x)\n",
    "    x = jax.checkpoint(g)(W3, x)\n",
    "    return x\n",
    "\n",
    "W1 = jnp.ones((5, 4))\n",
    "W2 = jnp.ones((6, 5))\n",
    "W3 = jnp.ones((7, 6))\n",
    "x = jnp.ones(4)\n",
    "\n",
    "jax.ad_checkpoint.print_saved_residuals(f, W1, W2, W3, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_checkpoint(x):\n",
    "  y = jax.checkpoint(g)(x)\n",
    "  z = h(y)\n",
    "  return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, you apply jax.checkpoint() to g — the first stage of f — rather than to f itself. This way, when you evaluate jax.grad(f_checkpoint)(x), you’d get a computation like:\n",
    "\n",
    "    Run the forward pass of g, discarding residual values.\n",
    "\n",
    "    Run the forward pass of h, saving residuals.\n",
    "\n",
    "    Run the backward pass of h, consuming residuals from step 2.\n",
    "\n",
    "    Re-run the forward pass of g, saving residuals.\n",
    "\n",
    "    Run the backward pass of g, consuming residuals from step 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, jax.checkpoint(foo) is a new function which has the same input-output behavior as foo, but behaves differently under autodiff, particularly under jax.linearize() and jax.vjp() (and their wrappers, like jax.grad()) but not jax.jvp(). When differentiated, only the input to a jax.checkpoint()-differentiated function is stored on the forward pass. On the backward pass, the residuals (intermediates from foo and its Jacobian coefficient values needed for the backward pass) are recomputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, when you use jax.checkpoint and reverse autodiff, during the forward pass, only the input to foo, x is stored. When you run jax.grad or jax.vjp, then the intermediates of foo are recomputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    Without jax.checkpoint(), JAX’s autodiff tends to compute everything possible on the forward pass and store it for the backward pass.\n",
    "\n",
    "    With a jax.checkpoint() decorator, you instead compute as little as possible on the forward pass and recompute values as needed on the backward pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even define policies on what to save and what to recompute. You can also define a policy wherein certain computations are offloaded to cpu memory once the computation is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that XLA does this save vs recompute automatically when using jax.jit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
