{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pallas is an extension to jax that enables writing custom kernels for GPUs and TPUs. It aims to provide fine-grained control over generated code, combining with the high-level ergonomics of JAX tracing and jax.numpy API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pallas requires users to think of memory access and how to divide computation across multiple compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On GPUs, pallas lowers to triton, and on TPUs, pallas lowers to Mosaic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.experimental.pallas as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel to add two vectors\n",
    "def add_vectors_kernel(x_ref, y_ref, o_ref):\n",
    "    x, y = x_ref[...], y_ref[...]\n",
    "    o_ref[...] = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs are Ref objects, which are basically pointers.\n",
    "# Function does not return anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Ref object with the ellipsis [...] means reading the whole array. Alternatively, you could also use [:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pallas_call function to launch the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: Triton support is only enabled for Ampere GPUs (compute capability 8.0) and up, but got compute capability 7.5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_vectors\u001b[39m(x: jax\u001b[38;5;241m.\u001b[39mArray, y: jax\u001b[38;5;241m.\u001b[39mArray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jax\u001b[38;5;241m.\u001b[39mArray:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pl\u001b[38;5;241m.\u001b[39mpallas_call(\n\u001b[1;32m      4\u001b[0m         add_vectors_kernel,\n\u001b[1;32m      5\u001b[0m         out_shape \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mShapeDtypeStruct(x\u001b[38;5;241m.\u001b[39mshape, x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m      6\u001b[0m     )(x, y)\n\u001b[0;32m----> 8\u001b[0m \u001b[43madd_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Notice that unlike triton, you do not need to allocate output memory and pass it as ptr. Instead, pallas_call just needs to know the shape and dtype. \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# It will create a buffer, pass it as the last arg, and fetch it back and return it for you.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/LinuxSetup/work/Learning-JAX/venv/lib/python3.10/site-packages/jax/_src/compiler.py:303\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    298\u001b[0m         built_c, compile_options\u001b[38;5;241m=\u001b[39moptions, host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    301\u001b[0m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xc\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    305\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: Triton support is only enabled for Ampere GPUs (compute capability 8.0) and up, but got compute capability 7.5."
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def add_vectors(x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    return pl.pallas_call(\n",
    "        add_vectors_kernel,\n",
    "        out_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n",
    "    )(x, y)\n",
    "\n",
    "add_vectors(jnp.arange(8), jnp.arange(8))\n",
    "\n",
    "# Notice that unlike triton, you do not need to allocate output memory and pass it as ptr. Instead, pallas_call just needs to know the shape and dtype. \n",
    "# It will create a buffer, pass it as the last arg, and fetch it back and return it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iota_kernel(o_ref):\n",
    "  i = pl.program_id(0)\n",
    "  o_ref[i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iota(size: int):\n",
    "  return pl.pallas_call(iota_kernel,\n",
    "                        out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),\n",
    "                        grid=(size,))()\n",
    "iota(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_kernel(x_ref, y_ref, z_ref):\n",
    "  z_ref[...] = x_ref[...] @ y_ref[...]\n",
    "\n",
    "def matmul(x: jax.Array, y: jax.Array):\n",
    "  return pl.pallas_call(\n",
    "    matmul_kernel,\n",
    "    out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),\n",
    "    grid=(2, 2),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)),\n",
    "        pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec(\n",
    "        (x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j),\n",
    "    )\n",
    "  )(x, y)\n",
    "k1, k2 = jax.random.split(jax.random.key(0))\n",
    "x = jax.random.normal(k1, (1024, 1024))\n",
    "y = jax.random.normal(k2, (1024, 1024))\n",
    "z = matmul(x, y)\n",
    "np.testing.assert_allclose(z, x @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid is just a tuple. And is same as CUDA. Each grid has the specified number of threadblocks in it.\n",
    "# Block is defined as pl.BlockSpec, again, just a tuple. However, here there is no restriction on the number of elements per block (since it'll be split\n",
    "# into 1024 threads at max).\n",
    "# Pallas also operates on vectors like Triton. Do not think in threads and warps. Only think blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can transform a pallas function with vmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the blockIdx, you can use jax.experiemental.pallas.program_id(axis=0|1|2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pallas.num_programs() to get the grid size of a given axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
